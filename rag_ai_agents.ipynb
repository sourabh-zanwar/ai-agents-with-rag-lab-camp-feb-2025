{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NG4czbO_KlM"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGOXphin_KlN"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade langchain_community langchain_core langchain_openai langgraph pypdf langchain_groq fastembed chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "# Path to the zip file\n",
        "zip_path = \"data.zip\"\n",
        "\n",
        "# Destination folder\n",
        "extract_to = \"./data\"\n",
        "\n",
        "shutil.unpack_archive(zip_path, extract_to)\n",
        "\n",
        "print(\"Files extracted successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from data.helper_functions import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "809cvF6c_KlP"
      },
      "source": [
        "# Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcvWg1lk_KlP"
      },
      "source": [
        "## LangChain and LangGraph\n",
        "\n",
        "### [LangChain Introduction](https://python.langchain.com/docs/introduction/)\n",
        "\n",
        "LangChain implements a standard interface for large language models and related technologies, such as embedding models and vector stores, and integrates with hundreds of providers.\n",
        "\n",
        "\n",
        "```python\n",
        "    from langchain.chat_models import init_chat_model\n",
        "    model = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")\n",
        "    model.invoke(\"Hello, world!\")\n",
        "```\n",
        "\n",
        "\n",
        "### [LangGraph Introduction](https://langchain-ai.github.io/langgraph/)\n",
        "\n",
        "Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features.\n",
        "\n",
        "```python\n",
        "    from typing import Annotated\n",
        "    from typing_extensions import TypedDict\n",
        "    from langgraph.graph import StateGraph, START, END\n",
        "    from langgraph.graph.message import add_messages\n",
        "    from langchain_anthropic import ChatAnthropic\n",
        "    llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n",
        "    class State(TypedDict):\n",
        "        messages: Annotated[list, add_messages]\n",
        "    def chatbot(state: State):\n",
        "        return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
        "    graph_builder = StateGraph(State)\n",
        "    graph_builder.add_node(\"chatbot\", chatbot)\n",
        "    graph_builder.add_edge(START, \"chatbot\")\n",
        "    graph_builder.add_edge(\"chatbot\", END)\n",
        "```\n",
        "\n",
        "![Basic LangGraph](./data/imgs/langgraph_basic.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaGDhXfC_KlQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"] = groq_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS_muQy8_KlQ"
      },
      "source": [
        "## AI Agents\n",
        "\n",
        "![AI Agents](./data/imgs/AI_Agents.jpg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqmmpedV_KlQ"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph.message import AnyMessage, add_messages\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], add_messages]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkLHxycc_KlR"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import Runnable, RunnableConfig\n",
        "\n",
        "\n",
        "class Assistant:\n",
        "    def __init__(self, runnable: Runnable):\n",
        "        self.runnable = runnable\n",
        "\n",
        "    def __call__(self, state: State, config: RunnableConfig):\n",
        "        while True:\n",
        "            configuration = config.get(\"configurable\", {})\n",
        "            result = self.runnable.invoke(state)\n",
        "            # If the LLM happens to return an empty response, we will re-prompt it\n",
        "            # for an actual response.\n",
        "            if not result.tool_calls and (\n",
        "                not result.content\n",
        "                or isinstance(result.content, list)\n",
        "                and not result.content[0].get(\"text\")\n",
        "            ):\n",
        "                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n",
        "                state = {**state, \"messages\": messages}\n",
        "            else:\n",
        "                break\n",
        "        return {\"messages\": result}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsbyiFzp_KlR"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def get_traffic_conditions(config: RunnableConfig, city):\n",
        "    \"\"\"\n",
        "    Get traffic conditions for a given city.\n",
        "    \"\"\"\n",
        "    traffic_conditions = {\n",
        "        'berlin': 'normal',\n",
        "        'munich': 'very busy',\n",
        "        'frankfurt': 'very busy'\n",
        "    }\n",
        "    return traffic_conditions[city.lower()]\n",
        "\n",
        "@tool\n",
        "def get_weather_conditions(config: RunnableConfig, city):\n",
        "    \"\"\"\n",
        "    Get weather conditions for a given city.\n",
        "    \"\"\"\n",
        "    weather_conditions = {\n",
        "        'berlin': '5C, rainy',\n",
        "        'munich': '2C, windy',\n",
        "        'frankfurt': '-1C, snowy'\n",
        "    }\n",
        "    return weather_conditions[city.lower()]\n",
        "\n",
        "\n",
        "@tool()\n",
        "def retrieve(config: RunnableConfig, query: str):\n",
        "    \"\"\"Retrieve information related to a query.\"\"\"\n",
        "    docs = vector_store.similarity_search(query, k=2)\n",
        "    retrived_docs = \"\\n\\n\".join(\n",
        "            (f\"Source: {doc.metadata['source']}\\n\" f\"Content: {doc.page_content}\")\n",
        "            for doc in docs\n",
        "    )\n",
        "    return retrived_docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5eNFcSk_KlR"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "llm = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
        "\n",
        "primary_assistant_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant who provides traffic and weather information\"\n",
        "            \"Use the provided tools to get the traffic and weather conditions for a city.\"\n",
        "        ),\n",
        "        (\"placeholder\", \"{messages}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "tools = [\n",
        "    get_traffic_conditions,\n",
        "    get_weather_conditions,\n",
        "    retrieve\n",
        "]\n",
        "\n",
        "assistant_runnable = primary_assistant_prompt | llm.bind_tools(tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YTi-Yxi_KlS"
      },
      "outputs": [],
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import END, StateGraph, START\n",
        "from langgraph.prebuilt import tools_condition\n",
        "\n",
        "builder = StateGraph(State)\n",
        "\n",
        "\n",
        "# Define nodes: these do the work\n",
        "builder.add_node(\"assistant\", Assistant(assistant_runnable))\n",
        "builder.add_node(\"tools\", create_tool_node_with_fallback(tools))\n",
        "# Define edges: these determine how the control flow moves\n",
        "builder.add_edge(START, \"assistant\")\n",
        "builder.add_conditional_edges(\n",
        "    \"assistant\",\n",
        "    tools_condition,\n",
        ")\n",
        "builder.add_edge(\"tools\", \"assistant\")\n",
        "\n",
        "# The checkpointer lets the graph persist its state\n",
        "# this is a complete memory for the entire graph.\n",
        "memory = MemorySaver()\n",
        "graph = builder.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "59DJs1RN_KlS",
        "outputId": "f452744d-68ba-46f4-9a62-45d3d8eb6a17"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
        "except Exception:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Te1jzpvb_KlT"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "thread_id = str(uuid.uuid4())\n",
        "config = {\n",
        "    \"configurable\": {\n",
        "        # Checkpoints are accessed by thread_id\n",
        "        \"thread_id\": thread_id,\n",
        "    }\n",
        "}\n",
        "\n",
        "questions = [\n",
        "    \"Can you tell me the traffic conditions in Berlin?\",\n",
        "    \"I want to travel from munich to frankfurt, do I need an umbrella?\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vROppvGB_KlT",
        "outputId": "0ee98380-76ce-4faa-8456-889785b8f201"
      },
      "outputs": [],
      "source": [
        "_printed = set()\n",
        "for question in questions:\n",
        "    events = graph.stream(\n",
        "        {\"messages\": (\"user\", question)}, config, stream_mode=\"values\"\n",
        "    )\n",
        "    for event in events:\n",
        "        _print_event(event, _printed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCLiLQhx_KlT"
      },
      "source": [
        "## Retrival Augmented Generation (RAG)\n",
        "\n",
        "![Retrival Augmented Generation](./data/imgs/RAG.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sijTgK7V_KlU"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
        "import os\n",
        "from glob import glob\n",
        "from langchain_groq import ChatGroq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FfcvlRG_KlU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"] = groq_key\n",
        "\n",
        "llm = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
        "\n",
        "embeddings = FastEmbedEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
        "\n",
        "vector_store = InMemoryVectorStore(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-0IodBh_KlU"
      },
      "outputs": [],
      "source": [
        "def load_documents(document_path):\n",
        "    loader = PyPDFLoader(document_path)\n",
        "    pages = []\n",
        "    for page in loader.lazy_load():\n",
        "      pages.append(page)\n",
        "    _ = vector_store.add_documents(pages)\n",
        "    print(f\"Loaded {len(pages)} pages from {document_path}\")\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLYl_PmABIZB"
      },
      "outputs": [],
      "source": [
        "load_documents('/content/rag_document.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKr_dv6zPoD2"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool()\n",
        "def retrieve(config: RunnableConfig, query: str):\n",
        "    \"\"\"Retrieve information related to a query.\"\"\"\n",
        "    docs = vector_store.similarity_search(query, k=2)\n",
        "    retrived_docs = \"\\n\\n\".join(\n",
        "            (f\"Source: {doc.metadata['source']}\\n\" f\"Content: {doc.page_content}\")\n",
        "            for doc in docs\n",
        "    )\n",
        "    return retrived_docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rCTVeFePoD5"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "llm = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
        "\n",
        "primary_assistant_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You also answer user's queries based only on the context that you get from the tools.\"\n",
        "            \"Include the source of the retrived context\"\n",
        "        ),\n",
        "        (\"placeholder\", \"{messages}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "tools = [\n",
        "    retrieve\n",
        "]\n",
        "\n",
        "assistant_runnable = primary_assistant_prompt | llm.bind_tools(tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDj5G5nLPoD5"
      },
      "outputs": [],
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import END, StateGraph, START\n",
        "from langgraph.prebuilt import tools_condition\n",
        "\n",
        "builder = StateGraph(State)\n",
        "\n",
        "\n",
        "# Define nodes: these do the work\n",
        "builder.add_node(\"assistant\", Assistant(assistant_runnable))\n",
        "builder.add_node(\"tools\", create_tool_node_with_fallback(tools))\n",
        "# Define edges: these determine how the control flow moves\n",
        "builder.add_edge(START, \"assistant\")\n",
        "builder.add_conditional_edges(\n",
        "    \"assistant\",\n",
        "    tools_condition,\n",
        ")\n",
        "builder.add_edge(\"tools\", \"assistant\")\n",
        "\n",
        "# The checkpointer lets the graph persist its state\n",
        "# this is a complete memory for the entire graph.\n",
        "memory = MemorySaver()\n",
        "graph = builder.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "jRy9yEWjPoD6",
        "outputId": "2255fa3e-a80a-47ed-bec0-330db668e04b"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
        "except Exception:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOWtTurSPoD6"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "thread_id = str(uuid.uuid4())\n",
        "config = {\n",
        "    \"configurable\": {\n",
        "        # Checkpoints are accessed by thread_id\n",
        "        \"thread_id\": thread_id,\n",
        "    }\n",
        "}\n",
        "\n",
        "questions = [\n",
        "    \"WHat are machine learning algorithms?\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49QE2UWkPoD7",
        "outputId": "0843d054-06f8-4bb2-8308-3f59a6e9651f"
      },
      "outputs": [],
      "source": [
        "_printed = set()\n",
        "for question in questions:\n",
        "    events = graph.stream(\n",
        "        {\"messages\": (\"user\", question)}, config, stream_mode=\"values\"\n",
        "    )\n",
        "    for event in events:\n",
        "        _print_event(event, _printed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBhUVoxt_KlV"
      },
      "source": [
        "# Improving Agent with RAG\n",
        "\n",
        "![AI Agents with RAG](./data/imgs/RAG_Agent.jpg)\n",
        "\n",
        "We bring all of the above together, with the following use case.\n",
        "\n",
        "\n",
        "## Use Case: AI Agent Assistant for modifying/booking flight tickets\n",
        "The agent we build should be able to do the following:\n",
        "- the agent should be able to book tickets\n",
        "- for rescheduling and cancelling the flights, it should reference the documentation for checking the terms and conditions\n",
        "- the agent should inform the user about the refund/additional charges, and confirm the changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ne198ZMV3No"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
        "import os\n",
        "from glob import glob\n",
        "from langchain_groq import ChatGroq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qP1yVZlWV3Np"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"] = groq_key\n",
        "\n",
        "llm = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
        "\n",
        "embeddings = FastEmbedEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
        "\n",
        "vector_store = InMemoryVectorStore(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8AjAKKsV3Np"
      },
      "outputs": [],
      "source": [
        "def load_documents(document_path):\n",
        "    loader = PyPDFLoader(document_path)\n",
        "    pages = []\n",
        "    for page in loader.lazy_load():\n",
        "      pages.append(page)\n",
        "    _ = vector_store.add_documents(pages)\n",
        "    print(f\"Loaded {len(pages)} pages from {document_path}\")\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FClwrh0eQCyM"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph.message import AnyMessage, add_messages\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], add_messages]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKeZlJLHQCyN"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import Runnable, RunnableConfig\n",
        "\n",
        "\n",
        "class Assistant:\n",
        "    def __init__(self, runnable: Runnable):\n",
        "        self.runnable = runnable\n",
        "\n",
        "    def __call__(self, state: State, config: RunnableConfig):\n",
        "        while True:\n",
        "            configuration = config.get(\"configurable\", {})\n",
        "            result = self.runnable.invoke(state)\n",
        "            # If the LLM happens to return an empty response, we will re-prompt it\n",
        "            # for an actual response.\n",
        "            if not result.tool_calls and (\n",
        "                not result.content\n",
        "                or isinstance(result.content, list)\n",
        "                and not result.content[0].get(\"text\")\n",
        "            ):\n",
        "                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n",
        "                state = {**state, \"messages\": messages}\n",
        "            else:\n",
        "                break\n",
        "        return {\"messages\": result}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wp3GVYOIQCyO"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "import sqlite3\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "db = 'flights.db'\n",
        "\n",
        "@tool\n",
        "def get_ticket_info(config: RunnableConfig, ticket_details):\n",
        "    conn = sqlite3.connect(db)\n",
        "    cursor = conn.cursor()\n",
        "    if 'ticket_number' in ticket_details.keys():\n",
        "      ticket_number = ticket_details['ticket_number']\n",
        "      query = f\"\"\"\n",
        "      SELECT\n",
        "          f.flight_number,\n",
        "          f.departure_airport,\n",
        "          f.arrival_airport,\n",
        "          f.departure,\n",
        "          f.arrival,\n",
        "          t.passenger_name,\n",
        "          t.ticket_number,\n",
        "          t.status,\n",
        "          t.booking_ts\n",
        "      FROM\n",
        "          tickets t\n",
        "          JOIN flights f ON t.flight_number = f.flight_number\n",
        "      WHERE\n",
        "          LOWER(t.ticket_number) = '{ticket_number.lower()}'\n",
        "      \"\"\"\n",
        "    elif 'passenger_name' in ticket_details.keys():\n",
        "      passenger_name = ticket_details['passenger_name']\n",
        "      query = f\"\"\"\n",
        "      SELECT\n",
        "          f.flight_number,\n",
        "          f.departure_airport,\n",
        "          f.arrival_airport,\n",
        "          f.departure,\n",
        "          f.arrival,\n",
        "          t.passenger_name,\n",
        "          t.ticket_number,\n",
        "          t.status,\n",
        "          t.booking_ts\n",
        "      FROM\n",
        "          tickets t\n",
        "          JOIN flights f ON t.flight_number = f.flight_number\n",
        "      WHERE\n",
        "          LOWER(t.passenger_name) = '{passenger_name.lower()}' AND t.status = 'BOOKED'\n",
        "      \"\"\"\n",
        "\n",
        "    cursor.execute(query)\n",
        "    rows = cursor.fetchall()\n",
        "    column_names = [column[0] for column in cursor.description]\n",
        "    results = [dict(zip(column_names, row)) for row in rows]\n",
        "\n",
        "    cursor.close()\n",
        "    conn.close()\n",
        "\n",
        "    return results\n",
        "\n",
        "@tool\n",
        "def get_flight_info(config: RunnableConfig, ticket_details):\n",
        "    \"\"\"\n",
        "    Get flight information\n",
        "    \"\"\"\n",
        "    departure_airport = ticket_details['from']\n",
        "    arrival_airport = ticket_details['to']\n",
        "    conn = sqlite3.connect(db)\n",
        "    cursor = conn.cursor()\n",
        "    query= f\"\"\"\n",
        "    SELECT *\n",
        "    FROM flights\n",
        "    WHERE LOWER(departure_airport) = '{departure_airport.lower()}' AND LOWER(arrival_airport) = '{arrival_airport.lower()}'\n",
        "    \"\"\"\n",
        "    cursor.execute(query)\n",
        "    rows = cursor.fetchall()\n",
        "    column_names = [column[0] for column in cursor.description]\n",
        "    results = [dict(zip(column_names, row)) for row in rows]\n",
        "\n",
        "    cursor.close()\n",
        "    conn.close()\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "@tool\n",
        "def book_ticket(config: RunnableConfig, ticket_details):\n",
        "    \"\"\"\n",
        "    Book a ticket.\n",
        "    \"\"\"\n",
        "    ticket_prefix = {\n",
        "      \"economy\": \"ECO\",\n",
        "      \"business\": \"BUS\",\n",
        "      \"flexi\": \"FLX\"\n",
        "    }\n",
        "    now = datetime.now()\n",
        "    flight_number = ticket_details['flight_number']\n",
        "    ticket_type = ticket_details['ticket_type']\n",
        "    passenger_name = ticket_details['passenger_name']\n",
        "    if ticket_type in ticket_prefix.keys():\n",
        "      ticket_type = ticket_prefix[ticket_type]\n",
        "    else:\n",
        "      ticket_type = \"UNK\"\n",
        "    ticket_number = ticket_type + \"_\" + generate_alphanumeric_code(8).upper()\n",
        "    conn = sqlite3.connect(db)\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    query = f\"\"\"\n",
        "    INSERT INTO tickets (flight_number, passenger_name, ticket_number, status, booking_ts)\n",
        "        VALUES ('{flight_number}', '{passenger_name}', '{ticket_number}', 'BOOKED', '{now.strftime(\"%m/%d/%Y, %H:%M:%S\")}')\n",
        "    \"\"\"\n",
        "    cursor.execute(query)\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    results = {\n",
        "        \"flight_number\": flight_number,\n",
        "        \"ticket_number\": ticket_number,\n",
        "        \"booking_time\": now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
        "    }\n",
        "    return results\n",
        "\n",
        "@tool\n",
        "def cancel_ticket(config: RunnableConfig, ticket_details):\n",
        "    \"\"\"\n",
        "    Cancel a ticket.\n",
        "    \"\"\"\n",
        "    now = datetime.now()\n",
        "    ticket_number = ticket_details['ticket_number']\n",
        "    conn = sqlite3.connect(db)\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    query = f\"\"\"\n",
        "    UPDATE tickets\n",
        "        SET status = 'CANCELLED'\n",
        "        WHERE ticket_number = '{ticket_number}'\n",
        "    \"\"\"\n",
        "    cursor.execute(query)\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    results = {\n",
        "        \"ticket_number\": ticket_number,\n",
        "        \"status\": \"CANCELLED\"\n",
        "    }\n",
        "    return results\n",
        "\n",
        "@tool()\n",
        "def retrieve_policy(config: RunnableConfig, query: str):\n",
        "    \"\"\"Retrieve information related to a query.\"\"\"\n",
        "    docs = vector_store.similarity_search(query, k=2)\n",
        "    retrived_docs = \"\\n\\n\".join(\n",
        "            (f\"Source: {doc.metadata['source']}\\n\" f\"Content: {doc.page_content}\")\n",
        "            for doc in docs\n",
        "    )\n",
        "    return retrived_docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZDjJSrRQCyP"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "llm = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
        "\n",
        "primary_assistant_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            #TODO: Write your prompt here\n",
        "        ),\n",
        "        (\"placeholder\", \"{messages}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "tools = [\n",
        "    #TODO: Pass the appropriate tools here\n",
        "]\n",
        "\n",
        "assistant_runnable = primary_assistant_prompt | llm.bind_tools(tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ko0BXbWHQCyP"
      },
      "outputs": [],
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import END, StateGraph, START\n",
        "from langgraph.prebuilt import tools_condition\n",
        "\n",
        "builder = StateGraph(State)\n",
        "\n",
        "\n",
        "# Define nodes: these do the work\n",
        "builder.add_node(\"assistant\", Assistant(assistant_runnable))\n",
        "builder.add_node(\"tools\", create_tool_node_with_fallback(tools))\n",
        "# Define edges: these determine how the control flow moves\n",
        "builder.add_edge(START, \"assistant\")\n",
        "builder.add_conditional_edges(\n",
        "    \"assistant\",\n",
        "    tools_condition,\n",
        ")\n",
        "builder.add_edge(\"tools\", \"assistant\")\n",
        "\n",
        "# The checkpointer lets the graph persist its state\n",
        "# this is a complete memory for the entire graph.\n",
        "memory = MemorySaver()\n",
        "graph = builder.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "JLOxoB7rQCyQ",
        "outputId": "f452744d-68ba-46f4-9a62-45d3d8eb6a17"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
        "except Exception:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoENnvvtQCyQ"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "thread_id = str(uuid.uuid4())\n",
        "config = {\n",
        "    \"configurable\": {\n",
        "        # Checkpoints are accessed by thread_id\n",
        "        \"thread_id\": thread_id,\n",
        "    }\n",
        "}\n",
        "\n",
        "questions = [\n",
        "    \"My name is Charlie Davis, can you show me my flight bookings?\",\n",
        "    \"Show me flights from Frankfurt to New York\",\n",
        "    \"Book a ticket in flight no FL1007, under the name Charlie Davis in Business class\",\n",
        "    \"Show me the bookings for Charlie Davis now.\",\n",
        "    \"Can I cancel my ticket with ticket number: {ENTER YOUR TICKET NUMBER HERE}\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7X1Z9tBQCyR"
      },
      "outputs": [],
      "source": [
        "_printed = set()\n",
        "for question in questions:\n",
        "    events = graph.stream(\n",
        "        {\"messages\": (\"user\", question)}, config, stream_mode=\"values\"\n",
        "    )\n",
        "    for event in events:\n",
        "        _print_event(event, _printed)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
